{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><Библиотека REQUEST><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "'Чешских крон'\n"
     ]
    }
   ],
   "source": [
    "#Перед началом работы библиотеку requests потребуется установить. !pip install requests \n",
    "#Познакомимся с библиотекой requests, решив простую задачу — получить значения курсов валют\n",
    "import requests # Импортируем библиотеку requests\n",
    "#url = 'https://www.cbr-xml-daily.ru/daily_json.js' # Определяем значение URL страницы для запроса (сайт с курасми валют)\n",
    "#response = requests.get(url) # Делаем GET-запрос к ресурсу и результат ответа сохраняем в переменной response\n",
    "#print(response) # получаем ответ 200лучае он равен 200 — то есть запрос был корректным и сервер отдал нам нужную информацию.\n",
    "# 404, означало бы, что страница по указанному адресу не найдена, а значение 403 — что синтаксис GET-запроса неверный.\n",
    "\n",
    "#Как получить доступ ко всей информации, которую содержит ответ? Текст ответа хранится в атрибуте text.\n",
    "#print(response.text)\n",
    "\n",
    "#Для того чтобы удобно было работать с полученной информацией, нам необходимо преобразовать строку в словарь.\n",
    "#В объект ответа Response  из библиотеки requests уже встроен метод json() .\n",
    "from pprint import pprint # Импортируем функцию pprint()\n",
    "currencies = response.json() # Применяем метод json()\n",
    "#pprint(currencies) # Выводим результат на экран)\n",
    " \n",
    "#Теперь данные находятся в словаре и можно легко получать необходимые значения.\n",
    "#Например, по ключу Valute мы можем обратиться к вложенному словарю, который содержит информацию о мировых валютах\n",
    "#pprint(currencies['Valute']['EUR'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПОЛУЧАЕМ СОДЕРЖИМОЕ ВЕБ-СТРАНИЦЫ. Библиотека BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей</title>\n",
      "Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей\n"
     ]
    }
   ],
   "source": [
    "import requests # Импортируем библиотеку requests\n",
    "url = 'https://nplus1.ru/news/2021/10/11/econobel2021' # Определяем адрес страницы\n",
    "response = requests.get(url)  # Выполняем GET-запрос\n",
    "#print(response.text)  # Выводим содержимое атрибута text\n",
    "#BeautifulSoup не является частью стандартной библиотеки, поэтому для начала её нужно установить. pip install beautifulsoup4 \n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://nplus1.ru/news/2021/10/11/econobel2021' # Определяем адрес страницы\n",
    "response = requests.get(url) # Выполняем GET-запрос, содержимое ответа присваивается переменной response\n",
    "page = BeautifulSoup(response.text, 'html.parser') # Создаём объект BeautifulSoup, указывая html-парсер\n",
    "print(page.title) # Получаем тег title, отображающийся на вкладке браузера\n",
    "print(page.title.text) # Выводим текст из полученного тега, который содержится в атрибуте text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если при запросе к сайту, а затем при его разборе с помощью BeautifulSoup в тексте страницы не находится нужный тег, попробуйте вывести на печать пару тысяч символов текста страницы. Если там обнаружится нечто похожее на капчу, возможно, сайт посчитал вас роботом и отказывается выдавать содержимое. Чтобы получить его, попробуйте «притвориться» браузером при запросе из скрипта:\n",
    "requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "User-Agent своего браузера можно узнать  - https://whatmyuseragent.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Премию Нобеля по экономике присудили за исследования экономики труда и причинно-следственных связей\n",
      "          \n",
      "Премия Шведского национального банка по экономическим наукам памяти Альфреда Нобеля за 2021 год присуждена Дэвиду Карду (David Card) за его вклад в эмпирические исследования экономики рынка труда, а также Джошуа Энгристу (Joshua Angrist) и Гвидо Имбенсу (Guido Imbens) за их вклад в методологию анализа причинно-следственных связей. Прямая трансляция церемонии объявления лауреатов шла на официальном сайте Нобелевской премии.\n",
      "\n",
      "11.10.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ИЗВЛЕКАЕМ ЗАГОЛОВОК И ВРЕМЯ НАПИСАНИЯ СТАТЬИ\n",
    "# Применяем метод find() к объекту и выводим результат на экран\n",
    "print(page.find('h1').text) #h1 это то, где находится заголовок. Посмотреть в коде страницы сайта: <h1>...<h1>\n",
    "print(page.find('div', class_='n1_material text-18').text) # Выводим текст, хотя его атрибут - <div>, добавлчем class, чтобв\n",
    "#именно этот текст, а не все <div>\n",
    "\n",
    "#Аналогично получим информации о теге, который содержит дату написания статьи, отображаемую в левом верхнем углу страницы.\n",
    "# Выводим на экран содержимое атрибута text тега a с классом \"relative\"\n",
    "print(page.find('a', class_= \"relative\").text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Напишите функцию wiki_header, которая по адресу страницы возвращает заголовок первого уровня для статей на Wikipedia.\n",
    "#Функция wiki_header принимает один аргумент - url.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def wiki_header(url):\n",
    "    page = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    header = page.find('h1').text\n",
    "    return header\n",
    "print (wiki_header('https://en.wikipedia.org/wiki/Operating_system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955\n",
      "['Machine code', 'MAD', 'MAD/I', 'Magik', 'Magma', 'Máni', 'Maple', 'MAPPER', 'MARK-IV', 'Mary']\n"
     ]
    }
   ],
   "source": [
    "#СБОР НЕСКОЛЬКИХ ЭЛЕМЕНТОВ: СОБИРАЕМ ВСЕ ССЫЛКИ НА СТРАНИЦЕ\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_programming_languages' # Задаём адрес ресурса\n",
    "response = requests.get(url) # Делаем GET-запрос к ресурсу\n",
    "page = BeautifulSoup(response.text, 'html.parser') # Создаём объект BeautifulSoup\n",
    "links = page.find_all('a') # Ищем все ссылки на странице и сохраняем в переменной links в виде списка\n",
    "print(len(links)) # Выводим количество найденных ссылок\n",
    "#на момент создания этих учебных материалов на странице содержалось 928 ссылок. Посмотрим на некоторые из них:\n",
    "print([link.text for link in links[500:510]]) # Выводим ссылки с 500 по 509 включительно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
